"""
å³æ™‚æ¼”è¬›ç¿»è­¯ç³»çµ±
æ¶æ§‹: ASR â†’ Translation â†’ Display/TTS
ä½¿ç”¨ faster-whisper + OpenAI/Claude API + å­—å¹•é¡¯ç¤º
"""

import warnings
# Suppress specific warnings
warnings.filterwarnings("ignore", message=".*Found GPU.*cuda capability.*")
warnings.filterwarnings("ignore", message=".*grouped_entities.*")

import pyaudio
import numpy as np
import threading
import queue
from faster_whisper import WhisperModel
from datetime import datetime
import time
import collections
from contextlib import contextmanager
import os
import sys
from vad import SileroVAD
from glossarymanager import GlossaryManager
from speakerid import SpeakerIdentifier
from punmarks import PunctuationRestorer
from translate import TranslationManager
from notifyserver import ServerNotifier
import argparse

@contextmanager
def ignore_stderr():
    """Suppress stderr output (useful for hiding ALSA/PyAudio warnings)"""
    try:
        devnull = os.open(os.devnull, os.O_WRONLY)
        old_stderr = os.dup(2)
        sys.stderr.flush()
        os.dup2(devnull, 2)
        os.close(devnull)
        try:
            yield
        finally:
            os.dup2(old_stderr, 2)
            os.close(old_stderr)
    except Exception:
        # If stderr redirection fails (e.g. on some Windows environments), just yield
        yield

class RealtimeSpeechTranslator:
    def __init__(self, 
                 source_lang="zh",
                 target_lang="en",
                 whisper_model="medium",
                 use_gpu=True,
                 model_dir=None,
                 enable_translate=False,
                 enable_translate=False,
                 trans_mode='local',
                 trans_url=None,
                 ollama_model=None):
        """
        åˆå§‹åŒ–å³æ™‚ç¿»è­¯ç³»çµ±
        
        Args:
            source_lang: ä¾†æºèªè¨€ (zh, en, ja, ko, etc.)
            target_lang: ç›®æ¨™èªè¨€
            whisper_model: Whisper æ¨¡å‹å¤§å° (tiny, base, small, medium, large)
            use_gpu: æ˜¯å¦ä½¿ç”¨ GPU
            model_dir: Whisper æ¨¡å‹ä¸‹è¼‰/è®€å–è·¯å¾‘ (Optional)
            enable_translate: æ˜¯å¦é–‹å•Ÿç¿»è­¯åŠŸèƒ½
            trans_mode: ç¿»è­¯æ¨¡å¼ ('local', 'remote', 'ollama')
            trans_url: é ç«¯ç¿»è­¯ API URL
            ollama_model: Ollama æ¨¡å‹åç¨±
        """
        # éŸ³è¨Šåƒæ•¸
        self.FORMAT = pyaudio.paInt16
        self.CHANNELS = 1
        self.RATE = 16000
        # VAD åƒæ•¸
        self.CHUNK_DURATION_MS = 32  # Silero VAD requires 32ms (512 samples) or 64ms (1024 samples) at 16k
        self.CHUNK_SIZE = int(self.RATE * self.CHUNK_DURATION_MS / 1000)  # 512 samples
        
        # Initialize Professional VAD
        self.vad_model = SileroVAD()
        
        # Initialize Speaker Identifier
        self.spk_id = SpeakerIdentifier()
        
        # Initialize Glossary Manager
        self.glossary = GlossaryManager()

        # Initialize Server Notifier
        self.notifier = ServerNotifier()
        
        # èªè¨€è¨­å®š
        self.source_lang = source_lang
        self.target_lang = target_lang
        
        # ç¿»è­¯è¨­å®š
        self.enable_translate = enable_translate
        self.trans_manager = TranslationManager(mode=trans_mode, url=trans_url, ollama_model=ollama_model) if enable_translate else None
        
        # è¼‰å…¥ Whisper æ¨¡å‹
        print(f"è¼‰å…¥ Whisper æ¨¡å‹: {whisper_model}")
        device = "cuda" if use_gpu else "cpu"
        compute_type = "float16" if use_gpu else "int8"
        
        try:
            self.whisper_model = WhisperModel(
                whisper_model, 
                device=device, 
                compute_type=compute_type,
                download_root=model_dir
            )
        except Exception as e:
            print(f"Whisper æ¨¡å‹è¼‰å…¥å¤±æ•—ï¼Œå˜—è©¦ä½¿ç”¨ CPU int8: {e}")
            self.whisper_model = WhisperModel(
                whisper_model, 
                device="cpu", 
                compute_type="int8",
                download_root=model_dir
            )
        
        # ä½‡åˆ—å’Œç‹€æ…‹
        self.audio_queue = queue.Queue()
        self.text_queue = queue.Queue()
        self.translation_queue = queue.Queue()
        self.running = False
        
        # VAD é–¾å€¼è¨­å®š (å‹•æ…‹èª¿æ•´æˆ–å›ºå®š)
        self.vad_threshold = 0.6      # Silero Probability threshold (Increased for stricter speech filtering)
        self.speech_pad_ms = 800      # èªéŸ³å‰å¾Œçš„ç·©è¡æ™‚é–“ (ms) - Increased for buffer
        self.min_speech_ms = 500      # æœ€çŸ­èªéŸ³é•·åº¦ (ms)
        self.max_silence_ms = 1200    # èªéŸ³ä¸­é–“å…è¨±çš„æœ€é•·éœéŸ³ (ms) - Increased to allow pauses
        
        # Dynamic Buffering Strategy
        self.dynamic_silence_ms = 600   # Aggressive silence threshold for long segments (ms)
        self.long_speech_ms = 15000     # Threshold to trigger aggressive completion (15s)
        self.force_speech_ms = 40000    # Hard limit to force cut (40s)
        
        # ç¿»è­¯ç·©å­˜ (é¿å…é‡è¤‡ç¿»è­¯)
        self.translation_cache = {}
        
    def _is_speech(self, audio_chunk):
        """è¨ˆç®— VAD Probability"""
        # audio_chunk æ˜¯ bytes
        # Using Silero VAD
        speech_prob = self.vad_model.is_speech(audio_chunk, self.RATE)
        return speech_prob > self.vad_threshold, speech_prob

    def audio_capture_thread(self):
        """
        åŸºæ–¼ VAD çš„æ™ºæ…§éŒ„éŸ³å¾ªç’°
        æµç¨‹: éœéŸ³ -> åµæ¸¬åˆ°è²éŸ³ -> éŒ„è£½ä¸­ -> éœéŸ³è¶…æ™‚ -> è¼¸å‡ºç‰‡æ®µ
        """
        with ignore_stderr():
            p = pyaudio.PyAudio()
        stream = p.open(
            format=self.FORMAT,
            channels=self.CHANNELS,
            rate=self.RATE,
            input=True,
            frames_per_buffer=self.CHUNK_SIZE
        )
        
        print(f"é–‹å§‹ç›£è½... (é–¾å€¼: {self.vad_threshold})")
        print("è«‹èªªè©±...")
        
        # ç‹€æ…‹è®Šæ•¸
        triggered = False
        frames = []
        silence_duration_ms = 0
        ring_buffer = collections.deque(maxlen=int(self.speech_pad_ms / self.CHUNK_DURATION_MS))
        
        while self.running:
            try:
                data = stream.read(self.CHUNK_SIZE, exception_on_overflow=False)
                is_speech, rms = self._is_speech(data)
                
                # ç°¡å–®çš„å‹•æ…‹é–¾å€¼èª¿æ•´ (å¯é¸ï¼Œé€™è£¡åªå°å‡º debug)
                # if not triggered and rms > 10: print(f"Current RMS: {rms:.1f}", end='\r')

                if not triggered:
                    ring_buffer.append(data)
                    if is_speech:
                        print(f"\n[åµæ¸¬åˆ°èªéŸ³] RMS: {rms:.1f} é–‹å§‹éŒ„è£½...")
                        triggered = True
                        frames.extend(ring_buffer) # åŠ å…¥ç·©è¡çš„å‰æ®µè²éŸ³
                        frames.append(data)
                        silence_duration_ms = 0
                else:
                    frames.append(data)
                    if is_speech:
                        silence_duration_ms = 0
                    else:
                        silence_duration_ms += self.CHUNK_DURATION_MS
                        
                    # Calculate current length
                    current_speech_len = len(frames) * self.CHUNK_DURATION_MS
                    
                    # Determine effective silence threshold based on length
                    effective_silence_threshold = self.max_silence_ms
                    if current_speech_len > self.long_speech_ms:
                        effective_silence_threshold = self.dynamic_silence_ms
                        
                    # Check for cut conditions:
                    # 1. Silence exceeded threshold
                    # 2. Total length exceeded hard limit
                    should_cut = (silence_duration_ms > effective_silence_threshold)
                    force_cut = (current_speech_len > self.force_speech_ms)
                    
                    if should_cut or force_cut:
                        reason = "Max silence" if should_cut else "Force cut"
                        print(f"[èªéŸ³çµæŸ] ({reason}) éŒ„è£½é•·åº¦: {current_speech_len / 1000:.2f}ç§’")
                        triggered = False
                        
                        # æª¢æŸ¥ç¸½é•·åº¦æ˜¯å¦è¶³å¤ 
                        total_duration_ms = len(frames) * self.CHUNK_DURATION_MS
                        if total_duration_ms > self.min_speech_ms:
                            # è¼¸å‡ºéŸ³è¨Š processing
                            audio_data = b''.join(frames)
                            np_audio = np.frombuffer(audio_data, dtype=np.int16).astype(np.float32) / 32768.0
                            self.audio_queue.put(np_audio)
                        else:
                            print("(èªéŸ³å¤ªçŸ­ï¼Œå¿½ç•¥)")
                            
                        # é‡ç½®
                        frames = []
                        ring_buffer.clear()
            
            except Exception as e:
                print(f"éŒ„éŸ³éŒ¯èª¤: {e}")
                break

        stream.stop_stream()
        stream.close()
        p.terminate()

    def asr_thread(self):
        """èªéŸ³è¾¨è­˜åŸ·è¡Œç·’"""
        print("ASR åŸ·è¡Œç·’å•Ÿå‹•")
        

        self.sentence_endings = {'ã€‚', 'ï¼Ÿ', 'ï¼', '.', '?', '!'}
        self.text_buffer = ""
        self.prev_text = ""  # ä¸Šä¸€å¥ç¢ºèªçš„æ–‡å­— (ç”¨ä½œ Prompt context)
        self.last_buffer_update = time.time()
        self.buffer_speaker = None
        
        # å˜—è©¦è¼‰å…¥æ¨™é»å¾©åŸæ¨¡å‹
        self.punct_restorer = PunctuationRestorer()

        while self.running:
            try:
                # å¾ä½‡åˆ—å–å¾—éŸ³è¨Š (Blocking)
                audio_data = self.audio_queue.get(timeout=1)
                
                print(f"æ­£åœ¨è¾¨è­˜... (é•·åº¦: {len(audio_data)/16000:.1f}s)")

                # Identify Speaker
                current_speaker = self.spk_id.identify(audio_data)
                self.last_known_speaker = current_speaker
                print(f"[{current_speaker}] æ­£åœ¨ç™¼è¨€...")
                
                # Check for speaker change
                if self.text_buffer and self.buffer_speaker and current_speaker != self.buffer_speaker:
                    print(f"\nğŸ” [Speaker Change] {self.buffer_speaker} -> {current_speaker}. Flushing buffer.")
                    
                    if self.punct_restorer.use_punct_model:
                        final_text = self.punct_restorer.restore(self.text_buffer)
                    else:
                        final_text = self.text_buffer
                        
                    final_text = final_text.strip()
                    if final_text:
                        timestamp = datetime.now().strftime("%H:%M:%S")
                        print(f"âœ… [{timestamp}] (Speaker Switch): {final_text}")
                        self.text_queue.put({
                            'text': final_text,
                            'speaker': self.buffer_speaker
                        })
                        self.prev_text = final_text
                    
                    self.text_buffer = ""
                
                if not self.text_buffer:
                    self.buffer_speaker = current_speaker
                
                # ä½¿ç”¨ Whisper è¾¨è­˜
                # åŠ å…¥ initial_prompt æä¾›ä¸Šä¸‹æ–‡ï¼Œæ¸›å°‘å¹»è¦ºä¸¦ç¶­æŒé€£è²«æ€§
                # çµåˆå‰æ–‡èˆ‡å°ˆæœ‰åè©
                glossary_prompt = self.glossary.get_prompt_context()
                prev_context = self.prev_text[-100:] if self.prev_text else "è«‹å°‡èªéŸ³è¾¨è­˜ç‚ºç¹é«”ä¸­æ–‡ã€‚"
                prompt = f"{glossary_prompt} {prev_context}".strip()
                
                segments, info = self.whisper_model.transcribe(
                    audio_data,
                    language=self.source_lang,
                    beam_size=5,
                    vad_filter=False,
                    initial_prompt=prompt
                )
                
                # Filter segments based on confidence to remove noise (coughing, throat clearing)
                valid_segments = []
                for segment in segments:
                    # no_speech_prob: Probability that the segment contains no speech
                    # avg_logprob: Average log probability (confidence) of the text
                    if segment.no_speech_prob > 0.95: 
                        print(f"ğŸ™ˆ éæ¿¾é›œéŸ³ (No Speech Prob: {segment.no_speech_prob:.2f}): {segment.text}")
                        continue
                    if segment.avg_logprob < -1.0: # Configurable threshold
                        print(f"ğŸ™ˆ éæ¿¾ä½ä¿¡åº¦ (LogProb: {segment.avg_logprob:.2f}): {segment.text}")
                        continue
                    valid_segments.append(segment.text.strip())

                # åˆä½µæ‰€æœ‰ç‰‡æ®µ
                current_text = " ".join(valid_segments)
                
                # æ‡‰ç”¨å°ˆæœ‰åè©æ ¡æ­£
                current_text = self.glossary.correct_text(current_text)
                current_text = self.glossary.clean_text(current_text)
                
                if current_text.strip():
                    print(f"ç‰‡æ®µè­˜åˆ¥: {current_text}")
                    self.text_buffer += current_text
                    self.last_buffer_update = time.time()
                    
                    # è™•ç†æ¨™é»èˆ‡æ–·å¥
                    restored_text = self.text_buffer
                    if self.punct_restorer.use_punct_model:
                        restored_text = self.punct_restorer.restore(self.text_buffer)
                    
                    is_complete = self.punct_restorer.is_complete_sentence(restored_text, self.sentence_endings)
                    
                    if is_complete:
                        final_text = restored_text.strip()
                        timestamp = datetime.now().strftime("%H:%M:%S")
                        print(f"\nâœ… [{timestamp}] : {final_text}")
                        
                        self.text_queue.put({
                            'text': final_text,
                            'speaker': current_speaker
                        })
                        self.prev_text = final_text
                        self.text_buffer = ""
                    else:
                        print(f"ç­‰å¾…å®Œæ•´å¥å­ (Restored: {restored_text})...")

                else:
                    print("âŒ ç„¡æ³•è­˜åˆ¥å‡ºæ–‡å­—")
                    
            except queue.Empty:
                # è¶…æ™‚æ©Ÿåˆ¶ï¼šå¦‚æœå¤ªä¹…æ²’æœ‰æ–°è²éŸ³(2ç§’)ï¼Œä¸”ç·©è¡å€æœ‰å­—ï¼Œå¼·åˆ¶è¼¸å‡º
                if self.text_buffer and (time.time() - self.last_buffer_update > 2.0):
                    
                    if self.punct_restorer.use_punct_model:
                        final_text = self.punct_restorer.restore(self.text_buffer)
                    else:
                        final_text = self.text_buffer
                        
                    final_text = final_text.strip()
                    if final_text:
                        timestamp = datetime.now().strftime("%H:%M:%S")
                        print(f"\nâ° [{timestamp}] è¶…æ™‚å¼·åˆ¶è¼¸å‡º: {final_text}")
                        # è¶…æ™‚å¼·åˆ¶è¼¸å‡ºæ™‚ï¼Œspeaker å¯èƒ½éœ€è¦ç”¨æœ€è¿‘ä¸€æ¬¡çš„ï¼Œæˆ– Unknown
                        # é€™è£¡æš«æ™‚ç„¡æ³•å–å¾—å®Œç¾çš„ speaker contextï¼Œè‹¥ audio capture thread æœ‰ä¿ç•™ speaker info æœƒæ›´å¥½
                        # ä½†æ—¢ç„¶æ˜¯ buffer æ®˜ç•™ï¼Œé€šå¸¸æ˜¯åŒä¸€å€‹äºº
                        # ç°¡åŒ–èµ·è¦‹ï¼Œé€™è£¡ä¸é‡æ–° identify (å› ç‚ºæ²’æœ‰ audio dataäº†)ï¼Œ
                        # æˆ‘å€‘å¯ä»¥å­˜ä¸€å€‹ self.last_speaker
                        
                        last_speaker = getattr(self, 'last_known_speaker', "Speaker ?")
                        self.text_queue.put({
                            'text': final_text,
                            'speaker': last_speaker
                        })
                        self.prev_text = final_text
                        self.text_buffer = ""
                continue
            except Exception as e:
                print(f"ASR éŒ¯èª¤: {e}")

    def translation_thread(self):
        """ç¿»è­¯åŸ·è¡Œç·’"""
        print("ç¿»è­¯åŸ·è¡Œç·’å•Ÿå‹•")
        
        while self.running:
            try:
                item = self.text_queue.get(timeout=1)
                if isinstance(item, dict):
                    text = item['text']
                    speaker = item.get('speaker', "Speaker ?")
                else:
                    text = item
                    speaker = "Speaker ?"


                # 1. æ¢å¾©åŸæœ¬çš„ç™¼é€å‹•ä½œ (Broadcast)
                self.notifier.send(text, speaker)

                # 2. ç¿»è­¯è™•ç† (Translation)
                if not self.enable_translate:
                    # å¦‚æœæœªé–‹å•Ÿç¿»è­¯ï¼Œç›´æ¥æ”¾å…¥éšŠåˆ—ä½†ç¿»è­¯æ¬„ä½ç‚ºç©ºæˆ–åŸæ¨£ï¼Œ
                    # ä½†æ ¹æ“šéœ€æ±‚: "è‹¥æ˜¯æ²’æœ‰é–‹å•Ÿç¿»è­¯åŠŸèƒ½ï¼Œå‰‡è¼¸å‡º è­¯æ–‡ çš„éƒ¨åˆ†ï¼Œä¸ç”¨é¡¯ç¤º"
                    # æˆ‘å€‘é€™è£¡å¯ä»¥è¨­ç‚º None
                    translation = None
                else:
                    translation = self.trans_manager.translate(text)
                
                timestamp = datetime.now().strftime("%H:%M:%S")
                
                self.translation_queue.put({
                    'original': text,
                    'translation': translation,
                    'speaker': speaker,
                    'timestamp': timestamp
                })
                
            except queue.Empty:
                continue
            except Exception as e:
                print(f"ç¿»è­¯éŒ¯èª¤: {e}")
    
    def display_thread(self):
        """é¡¯ç¤ºåŸ·è¡Œç·’"""
        print("é¡¯ç¤ºåŸ·è¡Œç·’å•Ÿå‹•")
        
        while self.running:
            try:
                result = self.translation_queue.get(timeout=1)
                
                print("\n" + "="*60)
                print(f"æ™‚é–“: {result['timestamp']}")
                print(f"è¬›è€…: {result['speaker']}")
                print(f"åŸæ–‡: {result['original']}")
                if result['translation']:
                    print(f"è­¯æ–‡: {result['translation']}")
                print("="*60 + "\n")
                
            except queue.Empty:
                continue
            except Exception as e:
                print(f"é¡¯ç¤ºéŒ¯èª¤: {e}")
    
    def start(self):
        """å•Ÿå‹•ç³»çµ±"""
        print("\n===== å°ˆæ¥­ç‰ˆå³æ™‚æ¼”è¬›ç¿»è­¯ç³»çµ± (VAD Enabled) =====")
        print(f"ä¾†æºèªè¨€: {self.source_lang}")
        print(f"ç›®æ¨™èªè¨€: {self.target_lang}")
        print(f"VAD é–¾å€¼: {self.vad_threshold}")
        print("æŒ‰ Ctrl+C åœæ­¢\n")
        
        self.running = True
        
        threads = [
            threading.Thread(target=self.audio_capture_thread, daemon=True),
            threading.Thread(target=self.asr_thread, daemon=True),
            threading.Thread(target=self.translation_thread, daemon=True),
            threading.Thread(target=self.display_thread, daemon=True)
        ]
        
        for thread in threads:
            thread.start()
        
        try:
            while True:
                time.sleep(0.1)
        except KeyboardInterrupt:
            print("\n\næ­£åœ¨åœæ­¢ç³»çµ±...")
            self.running = False
            for thread in threads:
                thread.join(timeout=2)
            print("ç³»çµ±å·²åœæ­¢")

if __name__ == "__main__":
    parser = argparse.ArgumentParser(description="Professional Realtime Speech Translator")
    
    # Mode arguments
    parser.add_argument("--model", type=str, default="medium", help="Whisper model size (small, medium, large-v2)")
    parser.add_argument("--model_dir", type=str, default=None, help="Path to model directory")
    parser.add_argument("--source", type=str, default="zh", help="Source language code")
    parser.add_argument("--target", type=str, default="en", help="Target language code")
    parser.add_argument("--gpu", action="store_true", default=True, help="Use GPU if available (default: True)")
    parser.add_argument("--no-gpu", action="store_false", dest="gpu", help="Force CPU usage")
    
    # Translation arguments
    parser.add_argument("--translate", action="store_true", help="Enable translation")
    parser.add_argument("--trans_mode", type=str, default="local", choices=["local", "remote", "ollama"], help="Translation mode: local, remote, or ollama")
    parser.add_argument("--trans_url", type=str, default=None, help="Remote translation URL (required for remote mode)")
    parser.add_argument("--ollama_model", type=str, default="hf.co/mradermacher/translategemma-12b-it-GGUF:Q4_K_M", help="Ollama model name")
    
    args = parser.parse_args()

    translator = RealtimeSpeechTranslator(
        source_lang=args.source,
        target_lang=args.target,
        whisper_model=args.model, 
        use_gpu=args.gpu,
        model_dir=args.model_dir,
        enable_translate=args.translate,
        trans_mode=args.trans_mode,
        trans_url=args.trans_url,
        ollama_model=args.ollama_model
    )
    translator.start()
